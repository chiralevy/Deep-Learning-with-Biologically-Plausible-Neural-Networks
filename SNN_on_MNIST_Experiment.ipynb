{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eff99ab5",
      "metadata": {
        "id": "eff99ab5"
      },
      "source": [
        "## Objective: Produce the Mean Performance of 10 SNNs and 10 ANNs on the MNIST Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDnIEHOKB8LD",
      "metadata": {
        "id": "hDnIEHOKB8LD"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from snntorch import spikeplot as splt\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "### DataLoading\n",
        "Define variables for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "data_path= \"raw\"\n",
        "#'/data/mnist'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# Torch Variables\n",
        "dtype = torch.float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3GdglZjK04cb",
      "metadata": {
        "id": "3GdglZjK04cb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0e7fce",
      "metadata": {
        "id": "ad0e7fce"
      },
      "outputs": [],
      "source": [
        "test_loader.dataset[0][0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "### Define Network with snnTorch. \n",
        "* `snn.Leaky()` instantiates a simple leaky integrate-and-fire neuron.\n",
        "* `spike_grad` optionally defines the surrogate gradient. If left undefined, the relevant gradient term is simply set to the output spike itself (1/0) by default.\n",
        "\n",
        "\n",
        "The problem with `nn.Sequential` is that each hidden layer can only pass one tensor to subsequent layers, whereas most spiking neurons return their spikes and hidden state(s). To handle this:\n",
        "\n",
        "* `init_hidden` initializes the hidden states (e.g., membrane potential) as instance variables to be processed in the background. \n",
        "\n",
        "The final layer is not bound by this constraint, and can return multiple tensors:\n",
        "* `output=True` enables the final layer to return the hidden state in addition to the spike."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "from snntorch import surrogate\n",
        "\n",
        "beta = 0.9  # neuron decay rate \n",
        "spike_grad = surrogate.fast_sigmoid()\n",
        "\n",
        "#  Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(1, 8, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(8, 16, 5), #8 in channels, 16 out channels, kernel of size 5\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(16*4*4, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c3fa47",
      "metadata": {
        "id": "11c3fa47"
      },
      "outputs": [],
      "source": [
        "#For testing purposes - Not included in experiment\n",
        "# net1 = nn.Sequential(nn.Conv2d(1, 8, 5),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Conv2d(8, 16, 5),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Conv2d(16, 16, 1),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Flatten(),\n",
        "#                     nn.Linear(16*2*2, 10),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "#                     ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sIrJnBoz490c",
      "metadata": {
        "id": "sIrJnBoz490c"
      },
      "source": [
        "### Define the Forward Pass\n",
        "Now define the forward pass over multiple time steps of simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWa8f_We4-8z",
      "metadata": {
        "id": "hWa8f_We4-8z"
      },
      "outputs": [],
      "source": [
        "from snntorch import utils \n",
        "\n",
        "def forward_pass(net, data, num_steps):  \n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps): \n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "  \n",
        "  return torch.stack(spk_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9nGhh2_25NU8",
      "metadata": {
        "id": "9nGhh2_25NU8"
      },
      "source": [
        "Define the optimizer and loss function. Here, we use the MSE Count Loss, which counts up the total number of output spikes at the end of the simulation run. The correct class has a target firing rate of 80% of all time steps, and incorrect classes are set to 20%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VocYbtD7Vwp7",
      "metadata": {
        "id": "VocYbtD7Vwp7"
      },
      "outputs": [],
      "source": [
        "import snntorch.functional as SF\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980c3e52",
      "metadata": {
        "id": "980c3e52"
      },
      "source": [
        "The accuracy on the full test set, again using `SF.accuracy_rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed3f101",
      "metadata": {
        "id": "9ed3f101"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    test_acc_hist = []\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec = forward_pass(net, data, num_steps)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "      test_acc_hist.append(acc)\n",
        "\n",
        "  return acc/total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48_7sIT86iUJ",
      "metadata": {
        "id": "48_7sIT86iUJ"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Now for the training loop. The predicted class will be set to the neuron with the highest firing rate, i.e., a rate-coded output. We will just measure accuracy on the training set. This training loop follows the same syntax as with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6c198e",
      "metadata": {
        "id": "ee6c198e"
      },
      "outputs": [],
      "source": [
        "num_replicates = 10\n",
        "num_epochs = 10\n",
        "num_steps = 25  # run for 25 time steps "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGZf7Hr55psl",
      "metadata": {
        "id": "kGZf7Hr55psl"
      },
      "outputs": [],
      "source": [
        "loss_hist = []\n",
        "acc_hist = []\n",
        "snn_val_list = []\n",
        "\n",
        "# training loop\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "          data = data.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          net.train()\n",
        "          spk_rec = forward_pass(net, data, num_steps)\n",
        "          loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Store loss history for future plotting\n",
        "          loss_hist.append(loss_val.item())\n",
        "\n",
        "          # print every 25 iterations\n",
        "          if i % 25 == 0:\n",
        "            print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "            # check accuracy on a single batch\n",
        "            acc = SF.accuracy_rate(spk_rec, targets)  \n",
        "            acc_hist.append(acc)\n",
        "            print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "          \n",
        "          # uncomment for faster termination\n",
        "          # if i == 150:\n",
        "          #     break\n",
        "        \n",
        "  snn_val_list.append(test_accuracy(test_loader, net, num_steps))\n",
        "\n",
        "print(f\"The average performance of this spiking neural network is {mean(snn_val_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Loss\n",
        "\n",
        "# fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "# plt.plot(loss_hist)\n",
        "# #plt.plot(test_loss_hist)\n",
        "# plt.title(\"Loss Curves\")\n",
        "# plt.legend([\"Train Loss\", \"Test Loss\"])\n",
        "# plt.xlabel(\"Iteration\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ngIFtQQDd7Hp"
      },
      "id": "ngIFtQQDd7Hp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# fig = plt.figure(facecolor=\"w\")\n",
        "# plt.plot(acc_hist_epoch)\n",
        "# plt.title(\"Test Set Accuracy\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "S4TV833M08bV"
      },
      "id": "S4TV833M08bV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f5544697",
      "metadata": {
        "id": "f5544697"
      },
      "source": [
        "## Training an a non-spiking ANN on MNIST (as provided by PyTorch)\n",
        "We will execute the following steps:\n",
        "1. Load and normalize the MNIST training and test datasets using torchvision\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "l4YefWYNG33W"
      },
      "id": "l4YefWYNG33W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31774a02",
      "metadata": {
        "id": "31774a02"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "num_replicates = 10\n",
        "\n",
        "# batch_size_train = 64\n",
        "# batch_size_test = 1000\n",
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "rUAgBrTfG3VS"
      },
      "id": "rUAgBrTfG3VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa862f2",
      "metadata": {
        "id": "2fa862f2"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Analogous to SNN above\n",
        "network = nn.Sequential(nn.Conv2d(1,8, 5),\n",
        "                    #out channel = in channel of subsequent layer\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.ReLU(),\n",
        "                    #snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(8, 16, 5), #8 in channels, 16 out channels, kernel of size 5\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.ReLU(),\n",
        "                    #snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(16*4*4, 10),\n",
        "                    #snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    nn.ReLU(),\n",
        "                    )\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "#         self.conv2_drop = nn.Dropout2d()\n",
        "#         self.fc1 = nn.Linear(320, 50)\n",
        "#         self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "#         x = x.view(-1, 320)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.dropout(x, training=self.training)\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f59070e",
      "metadata": {
        "id": "9f59070e"
      },
      "outputs": [],
      "source": [
        "#network = Net()\n",
        "\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,momentum=momentum)\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "\n",
        "criteria = torch.nn.CrossEntropyLoss()\n",
        "#criteria = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1645df",
      "metadata": {
        "id": "2d1645df"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0836c4b4",
      "metadata": {
        "id": "0836c4b4"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = criteria(output, target)\n",
        "    # loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      #torch.save(network.state_dict(), '/results/model.pth')\n",
        "      #torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216f301c",
      "metadata": {
        "id": "216f301c"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += criteria(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
        "  return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4509dd27",
      "metadata": {
        "id": "4509dd27"
      },
      "outputs": [],
      "source": [
        "ann_val_list = []\n",
        "#test()\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train(epoch)\n",
        "    #test()\n",
        "  ann_val_list.append(float(test()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The average performance of this artificial neural network: {mean(ann_val_list))}\")"
      ],
      "metadata": {
        "id": "cmeHgRVRt-n5"
      },
      "id": "cmeHgRVRt-n5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "144436cb",
      "metadata": {
        "id": "144436cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "#plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "fig"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SNN on MNIST Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}