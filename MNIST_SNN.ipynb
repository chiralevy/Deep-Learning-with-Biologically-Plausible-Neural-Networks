{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eff99ab5",
      "metadata": {},
      "source": [
        "## Goal: Train SNN on MNIST task + Compare performance with ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "hDnIEHOKB8LD",
      "metadata": {
        "id": "hDnIEHOKB8LD"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "## DataLoading\n",
        "Define variables for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "data_path= \"raw\"\n",
        "#'/data/mnist'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Torch Variables\n",
        "dtype = torch.float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3GdglZjK04cb",
      "metadata": {
        "id": "3GdglZjK04cb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "## Define Network with snnTorch. \n",
        "* `snn.Leaky()` instantiates a simple leaky integrate-and-fire neuron.\n",
        "* `spike_grad` optionally defines the surrogate gradient. If left undefined, the relevant gradient term is simply set to the output spike itself (1/0) by default.\n",
        "\n",
        "\n",
        "The problem with `nn.Sequential` is that each hidden layer can only pass one tensor to subsequent layers, whereas most spiking neurons return their spikes and hidden state(s). To handle this:\n",
        "\n",
        "* `init_hidden` initializes the hidden states (e.g., membrane potential) as instance variables to be processed in the background. \n",
        "\n",
        "The final layer is not bound by this constraint, and can return multiple tensors:\n",
        "* `output=True` enables the final layer to return the hidden state in addition to the spike."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "from snntorch import surrogate\n",
        "\n",
        "beta = 0.9  # neuron decay rate \n",
        "spike_grad = surrogate.fast_sigmoid()\n",
        "\n",
        "#  Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(1, 8, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(8, 16, 5),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(16*4*4, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sIrJnBoz490c",
      "metadata": {
        "id": "sIrJnBoz490c"
      },
      "source": [
        "## Define the Forward Pass\n",
        "Now define the forward pass over multiple time steps of simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "hWa8f_We4-8z",
      "metadata": {
        "id": "hWa8f_We4-8z"
      },
      "outputs": [],
      "source": [
        "from snntorch import utils \n",
        "\n",
        "def forward_pass(net, data, num_steps):  \n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps): \n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "  \n",
        "  return torch.stack(spk_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9nGhh2_25NU8",
      "metadata": {
        "id": "9nGhh2_25NU8"
      },
      "source": [
        "Define the optimizer and loss function. Here, we use the MSE Count Loss, which counts up the total number of output spikes at the end of the simulation run. The correct class has a target firing rate of 80% of all time steps, and incorrect classes are set to 20%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "VocYbtD7Vwp7",
      "metadata": {
        "id": "VocYbtD7Vwp7"
      },
      "outputs": [],
      "source": [
        "import snntorch.functional as SF\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48_7sIT86iUJ",
      "metadata": {
        "id": "48_7sIT86iUJ"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Now for the training loop. The predicted class will be set to the neuron with the highest firing rate, i.e., a rate-coded output. We will just measure accuracy on the training set. This training loop follows the same syntax as with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "kGZf7Hr55psl",
      "metadata": {
        "id": "kGZf7Hr55psl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Iteration 0 \n",
            "Train Loss: 2.45\n",
            "Accuracy: 12.50%\n",
            "\n",
            "Epoch 0, Iteration 25 \n",
            "Train Loss: 0.74\n",
            "Accuracy: 41.41%\n",
            "\n",
            "Epoch 0, Iteration 50 \n",
            "Train Loss: 0.57\n",
            "Accuracy: 60.16%\n",
            "\n",
            "Epoch 0, Iteration 75 \n",
            "Train Loss: 0.38\n",
            "Accuracy: 87.50%\n",
            "\n",
            "Epoch 0, Iteration 100 \n",
            "Train Loss: 0.32\n",
            "Accuracy: 90.62%\n",
            "\n",
            "Epoch 0, Iteration 125 \n",
            "Train Loss: 0.26\n",
            "Accuracy: 93.75%\n",
            "\n",
            "Epoch 0, Iteration 150 \n",
            "Train Loss: 0.27\n",
            "Accuracy: 92.19%\n",
            "\n",
            "Epoch 0, Iteration 175 \n",
            "Train Loss: 0.22\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 200 \n",
            "Train Loss: 0.22\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 225 \n",
            "Train Loss: 0.21\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 250 \n",
            "Train Loss: 0.20\n",
            "Accuracy: 93.75%\n",
            "\n",
            "Epoch 0, Iteration 275 \n",
            "Train Loss: 0.18\n",
            "Accuracy: 97.66%\n",
            "\n",
            "Epoch 0, Iteration 300 \n",
            "Train Loss: 0.18\n",
            "Accuracy: 96.09%\n",
            "\n",
            "Epoch 0, Iteration 325 \n",
            "Train Loss: 0.17\n",
            "Accuracy: 96.09%\n",
            "\n",
            "Epoch 0, Iteration 350 \n",
            "Train Loss: 0.16\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 375 \n",
            "Train Loss: 0.19\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 400 \n",
            "Train Loss: 0.19\n",
            "Accuracy: 95.31%\n",
            "\n",
            "Epoch 0, Iteration 425 \n",
            "Train Loss: 0.18\n",
            "Accuracy: 94.53%\n",
            "\n",
            "Epoch 0, Iteration 450 \n",
            "Train Loss: 0.18\n",
            "Accuracy: 93.75%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "num_steps = 25  # run for 25 time steps \n",
        "\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        net.train()\n",
        "        spk_rec = forward_pass(net, data, num_steps)\n",
        "        loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # print every 25 iterations\n",
        "        if i % 25 == 0:\n",
        "          print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "          # check accuracy on a single batch\n",
        "          acc = SF.accuracy_rate(spk_rec, targets)  \n",
        "          acc_hist.append(acc)\n",
        "          print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "        \n",
        "        # uncomment for faster termination\n",
        "        # if i == 150:\n",
        "        #     break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-ilZc_G-AUE",
      "metadata": {
        "id": "h-ilZc_G-AUE"
      },
      "source": [
        "Let's see the accuracy on the full test set, again using `SF.accuracy_rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "QYXr5reY95Lc",
      "metadata": {
        "id": "QYXr5reY95Lc"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec = forward_pass(net, data, num_steps)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "BRRm1QpG9w7N",
      "metadata": {
        "id": "BRRm1QpG9w7N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy: 97.960%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test set accuracy: {test_accuracy(test_loader, net, num_steps)*100:.3f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Copy of tutorial_5_neuromorphic_datasets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
