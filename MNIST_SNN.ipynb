{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eff99ab5",
      "metadata": {},
      "source": [
        "## Goal: Compare the Mean Performance of 10 ANNs and 10 SNNs on the MNIST Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDnIEHOKB8LD",
      "metadata": {
        "id": "hDnIEHOKB8LD"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "#From SNN\n",
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "from statistics import mean\n",
        "\n",
        "#From ANN\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "### DataLoading\n",
        "Define variables for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "data_path= \"raw\"\n",
        "#'/data/mnist'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Torch Variables\n",
        "dtype = torch.float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3GdglZjK04cb",
      "metadata": {
        "id": "3GdglZjK04cb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0e7fce",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader.dataset[0][0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "### Define Network with snnTorch. \n",
        "* `snn.Leaky()` instantiates a simple leaky integrate-and-fire neuron.\n",
        "* `spike_grad` optionally defines the surrogate gradient. If left undefined, the relevant gradient term is simply set to the output spike itself (1/0) by default.\n",
        "\n",
        "\n",
        "The problem with `nn.Sequential` is that each hidden layer can only pass one tensor to subsequent layers, whereas most spiking neurons return their spikes and hidden state(s). To handle this:\n",
        "\n",
        "* `init_hidden` initializes the hidden states (e.g., membrane potential) as instance variables to be processed in the background. \n",
        "\n",
        "The final layer is not bound by this constraint, and can return multiple tensors:\n",
        "* `output=True` enables the final layer to return the hidden state in addition to the spike."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "from snntorch import surrogate\n",
        "\n",
        "beta = 0.9  # neuron decay rate \n",
        "spike_grad = surrogate.fast_sigmoid()\n",
        "\n",
        "#  Initialize Network\n",
        "net = nn.Sequential(nn.Conv2d(1, 8, 5),\n",
        "                    #out channel = in channel of subsequent layer\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(8, 16, 5), #8 in channels, 16 out channels, kernel of size 5\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(16*4*4, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c3fa47",
      "metadata": {},
      "outputs": [],
      "source": [
        "#For Testing Purposes - Not included in experiment\n",
        "# net1 = nn.Sequential(nn.Conv2d(1, 8, 5),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Conv2d(8, 16, 5),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Conv2d(16, 16, 1),\n",
        "#                     nn.MaxPool2d(2),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "#                     nn.Flatten(),\n",
        "#                     nn.Linear(16*2*2, 10),\n",
        "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "#                     ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sIrJnBoz490c",
      "metadata": {
        "id": "sIrJnBoz490c"
      },
      "source": [
        "### Define the Forward Pass\n",
        "Now define the forward pass over multiple time steps of simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWa8f_We4-8z",
      "metadata": {
        "id": "hWa8f_We4-8z"
      },
      "outputs": [],
      "source": [
        "from snntorch import utils \n",
        "\n",
        "def forward_pass(net, data, num_steps):  \n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps): \n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "  \n",
        "  return torch.stack(spk_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9nGhh2_25NU8",
      "metadata": {
        "id": "9nGhh2_25NU8"
      },
      "source": [
        "Define the optimizer and loss function. Here, we use the MSE Count Loss, which counts up the total number of output spikes at the end of the simulation run. The correct class has a target firing rate of 80% of all time steps, and incorrect classes are set to 20%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VocYbtD7Vwp7",
      "metadata": {
        "id": "VocYbtD7Vwp7"
      },
      "outputs": [],
      "source": [
        "import snntorch.functional as SF\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "980c3e52",
      "metadata": {},
      "source": [
        "The accuracy on the full test set, again using `SF.accuracy_rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed3f101",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec = forward_pass(net, data, num_steps)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48_7sIT86iUJ",
      "metadata": {
        "id": "48_7sIT86iUJ"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Now for the training loop. The predicted class will be set to the neuron with the highest firing rate, i.e., a rate-coded output. We will just measure accuracy on the training set. This training loop follows the same syntax as with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6c198e",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "num_replicates = 2\n",
        "num_steps = 25  # run for 25 time steps "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGZf7Hr55psl",
      "metadata": {
        "id": "kGZf7Hr55psl"
      },
      "outputs": [],
      "source": [
        "loss_hist = []\n",
        "acc_hist = []\n",
        "snn_val_list = []\n",
        "\n",
        "# training loop\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "          data = data.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          net.train()\n",
        "          spk_rec = forward_pass(net, data, num_steps)\n",
        "          loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Store loss history for future plotting\n",
        "          loss_hist.append(loss_val.item())\n",
        "\n",
        "          # print every 25 iterations\n",
        "          if i % 25 == 0:\n",
        "            #print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "            # check accuracy on a single batch\n",
        "            acc = SF.accuracy_rate(spk_rec, targets)  \n",
        "            acc_hist.append(acc)\n",
        "            #print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "          \n",
        "          # uncomment for faster termination\n",
        "          # if i == 150:\n",
        "          #     break\n",
        "    \n",
        "  snn_val_list.append(test_accuracy(test_loader, net, num_steps))\n",
        "  \n",
        "print(snn_val_list)\n",
        "print(mean(snn_val_list))\n",
        "print(f\"the average performance of this artificial neural network is {mean(snn_val_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BRRm1QpG9w7N",
      "metadata": {
        "id": "BRRm1QpG9w7N"
      },
      "outputs": [],
      "source": [
        "print(f\"Test set accuracy: {test_accuracy(test_loader, net, num_steps)*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5544697",
      "metadata": {},
      "source": [
        "### Training an a non-spiking NN on MNIST (as provided by PyTorch)\n",
        "We will do the following steps in order:\n",
        "1. Load and normalize the MNIST training and test datasets using torchvision\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31774a02",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 3\n",
        "num_replicates = 2\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa862f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Working on making this analogous to SNN above\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "#         self.conv2_drop = nn.Dropout2d()\n",
        "#         self.fc1 = nn.Linear(320, 50)\n",
        "#         self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "#         x = x.view(-1, 320)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.dropout(x, training=self.training)\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f59070e",
      "metadata": {},
      "outputs": [],
      "source": [
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1645df",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0836c4b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      # torch.save(network.state_dict(), '/results/model.pth')\n",
        "      # torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216f301c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  #print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
        "  return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4509dd27",
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_val_list = []\n",
        "#test()\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train(epoch)\n",
        "    #test()\n",
        "  #nn_val_list.append(test())\n",
        "  nn_val_list.append(float(test()))\n",
        "print(nn_val_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "144436cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "#plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "#fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0dd552",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Copy of tutorial_5_neuromorphic_datasets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
