{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5fbb5fce",
      "metadata": {
        "id": "5fbb5fce"
      },
      "source": [
        "## Objective: Produce the Mean Performance of 10 SNNs and 10 ANNs on the CIFAR-10 Task\n",
        "+ A Surrogate CNN is trained via Backpropagation Through Time per the instructions in snnTorch Tutorial 6.\n",
        "+ A non-spiking ANN is trained below for comparision. Pipeline provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e3121b",
      "metadata": {
        "id": "95e3121b"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from snntorch import backprop\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "## DataLoading\n",
        "Define variables for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "subset = 1\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Torch Variables\n",
        "dtype = torch.float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load CIFAR dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b87c4c",
      "metadata": {
        "id": "b2b87c4c"
      },
      "outputs": [],
      "source": [
        "#Define a Transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "#cifar10_test = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
        "\n",
        "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98c329f",
      "metadata": {
        "id": "f98c329f"
      },
      "outputs": [],
      "source": [
        "test_loader.dataset[0][0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "## Define Network with snnTorch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "from snntorch import surrogate\n",
        "\n",
        "beta = 0.9  # neuron decay rate \n",
        "spike_grad = surrogate.fast_sigmoid(slope = 15)\n",
        "num_steps = 25\n",
        "\n",
        "#Initialize Network. To be Modified Later\n",
        "net = nn.Sequential(nn.Conv2d(3, 12, 3),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Conv2d(12, 64, 3),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(64*6*6, 512),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Linear(512, 128),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
        "                    nn.Linear(128, 10),\n",
        "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
        "                    ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sIrJnBoz490c",
      "metadata": {
        "id": "sIrJnBoz490c"
      },
      "source": [
        "## Define the Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWa8f_We4-8z",
      "metadata": {
        "id": "hWa8f_We4-8z"
      },
      "outputs": [],
      "source": [
        "from snntorch import utils\n",
        "\n",
        "def forward_pass(net, data, num_steps):\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "\n",
        "  return torch.stack(spk_rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9nGhh2_25NU8",
      "metadata": {
        "id": "9nGhh2_25NU8"
      },
      "source": [
        "Define the optimizer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VocYbtD7Vwp7",
      "metadata": {
        "id": "VocYbtD7Vwp7"
      },
      "outputs": [],
      "source": [
        "import snntorch.functional as SF\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
        "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
        "\n",
        "#alternative\n",
        "#loss_fn = SF.ce_rate_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b501c6d",
      "metadata": {
        "id": "3b501c6d"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(data_loader, net, num_steps):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    acc = 0\n",
        "    net.eval()\n",
        "\n",
        "    data_loader = iter(data_loader)\n",
        "    for data, targets in data_loader:\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "      spk_rec = forward_pass(net, data, num_steps)\n",
        "\n",
        "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "  return acc/total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48_7sIT86iUJ",
      "metadata": {
        "id": "48_7sIT86iUJ"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_replicates = 10\n",
        "num_epochs = 10\n",
        "num_steps = 25  # run for 25 time steps"
      ],
      "metadata": {
        "id": "q2vdCERj1n8O"
      },
      "id": "q2vdCERj1n8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QYXr5reY95Lc",
      "metadata": {
        "id": "QYXr5reY95Lc"
      },
      "outputs": [],
      "source": [
        "loss_hist = []\n",
        "acc_hist = []\n",
        "snn_val_list = []\n",
        "\n",
        "# training loop\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "          data = data.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          net.train()\n",
        "          spk_rec = forward_pass(net, data, num_steps)\n",
        "          loss_val = loss_fn(spk_rec, targets)\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Store loss history for future plotting\n",
        "          loss_hist.append(loss_val.item())\n",
        "\n",
        "          # print every 25 iterations\n",
        "          if i % 25 == 0:\n",
        "            print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
        "\n",
        "            # check accuracy on a single batch\n",
        "            acc = SF.accuracy_rate(spk_rec, targets)\n",
        "            acc_hist.append(acc)\n",
        "            print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "\n",
        "          # uncomment for faster termination\n",
        "          # if i == 150:\n",
        "          #     break\n",
        "\n",
        "  snn_val_list.append(test_accuracy(test_loader, net, num_steps))\n",
        "print(f\"The average performance of this spiking neural network is {mean(snn_val_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a44214",
      "metadata": {
        "id": "e9a44214"
      },
      "source": [
        "Training an a non-spiking ANN on CIFAR-10 (as provided by PyTorch).\n",
        "We will dexecute the following steps:\n",
        "\n",
        "1. Load and normalize the CIFAR10 training and test datasets using torchvision\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b51062",
      "metadata": {
        "id": "50b51062"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea0d2db",
      "metadata": {
        "id": "4ea0d2db"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc087028",
      "metadata": {
        "id": "dc087028"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e971ac87",
      "metadata": {
        "id": "e971ac87"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58dc3203",
      "metadata": {
        "id": "58dc3203"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 12, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(12, 64, 3)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 6, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53c12369",
      "metadata": {
        "id": "53c12369"
      },
      "source": [
        "3. Define a Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e1c080",
      "metadata": {
        "id": "76e1c080"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e67567",
      "metadata": {
        "id": "c8e67567"
      },
      "source": [
        "4. Train the network\n",
        "5. Test the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b70df9",
      "metadata": {
        "id": "80b70df9"
      },
      "outputs": [],
      "source": [
        "ann_val_list = []\n",
        "for replicate in range(num_replicates):\n",
        "  for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "              print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "          # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = net(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  #print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "  ann_val_list.append(100 * correct // total)\n",
        "print(f\"The average performance of this non-spiking artificial neural network is {mean(ann_val_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "rNRMPRCXn4Tj"
      },
      "id": "rNRMPRCXn4Tj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SNN on CIFAR10 Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}